{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "try:\n",
    "    import cPickle as thepickle\n",
    "except ImportError:\n",
    "    import _pickle as thepickle\n",
    "\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from new_model import create_model, create_model_2d\n",
    "import keras.backend.tensorflow_backend as ktf\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Dot\n",
    "from keras import optimizers\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session(gpu_fraction=0.85):\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction,\n",
    "                                allow_growth=True)\n",
    "    return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_whole_seq_new(tor_seq,exit_seq,circuit_labels,test_c,train_c,model_gb):\n",
    "    train_window1=[]\n",
    "    train_window2=[]\n",
    "    test_window1=[]\n",
    "    test_window2=[]\n",
    "    window_tor=[]\n",
    "    window_exit=[]\n",
    "\n",
    "    window_tor_size = []\n",
    "    window_exit_size = []\n",
    "    window_tor_ipd = []\n",
    "    window_exit_ipd = []\n",
    "    print(\"extract both ipd and size features...\")\n",
    "    for i in range(len(tor_seq)):\n",
    "        window_tor_size.append([float(pair[\"size\"])/1000.0 for pair in tor_seq[i]])\n",
    "        window_exit_size.append([float(pair[\"size\"]) / 1000.0 for pair in exit_seq[i]])\n",
    "        window_tor_ipd.append ([float(pair[\"ipd\"])* 1000.0 for pair in tor_seq[i]])\n",
    "        window_exit_ipd.append ([float(pair[\"ipd\"])* 1000.0 for pair in exit_seq[i]])\n",
    "\n",
    "    print('window_tor_size', np.array(window_tor_size).shape)\n",
    "    print('window_exit_size', np.array(window_exit_size).shape)\n",
    "    print('window_tor_ipd', np.array(window_tor_ipd).shape)\n",
    "    print('window_exit_ipd', np.array(window_exit_ipd).shape)\n",
    "    window_tor_ipd = np.array(window_tor_ipd)\n",
    "    window_exit_ipd = np.array(window_exit_ipd)\n",
    "\n",
    "    # Change the first idp to 0 across all windows.\n",
    "    new_window_tor_ipd = []\n",
    "    new_window_exit_ipd = []\n",
    "    for trace in window_tor_ipd:\n",
    "        new_trace = [0]+list(trace[1:])\n",
    "        new_window_tor_ipd.append([ipd for ipd in new_trace])\n",
    "    for trace in window_exit_ipd:\n",
    "        new_trace = [0]+list(trace[1:])\n",
    "        new_window_exit_ipd.append([ipd for ipd in new_trace])\n",
    "\n",
    "    window_tor_ipd = new_window_tor_ipd\n",
    "    window_exit_ipd = new_window_exit_ipd\n",
    "    print('window_tor_ipd',window_tor_ipd[10][:10])\n",
    "    print('window_exit_ipd',window_exit_ipd[10][:10])\n",
    "\n",
    "    for i in range(len(window_tor_ipd)):\n",
    "        window_tor.append(np.concatenate((window_tor_ipd[i], window_tor_size[i]), axis=None))\n",
    "        window_exit.append(np.concatenate((window_exit_ipd[i], window_exit_size[i]), axis=None))\n",
    "\n",
    "    window_tor = np.array(window_tor)\n",
    "    window_exit = np.array(window_exit)\n",
    "    print('window_tor', window_tor.shape)\n",
    "    print('window_exit', window_exit.shape)\n",
    "\n",
    "    for w, c in zip (window_tor, circuit_labels):\n",
    "        if c in train_c:\n",
    "            train_window1.append(w)\n",
    "        elif c in test_c:\n",
    "            test_window1.append(w)\n",
    "\n",
    "    for w, c in zip (window_exit, circuit_labels):\n",
    "        if c in train_c:\n",
    "            train_window2.append(w)\n",
    "        elif c in test_c:\n",
    "            test_window2.append(w)\n",
    "\n",
    "    print ('train_window1', np.array(train_window1).shape)\n",
    "    print ('train_window2', np.array(train_window1).shape)\n",
    "\n",
    "    return np.array(train_window1), np.array(train_window2), np.array(test_window1), np.array(test_window2), np.array(test_window1), np.array(test_window2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(argparse.Namespace):\n",
    "    tor_len = 500\n",
    "    win_interval = 5\n",
    "    exit_len = 800\n",
    "    num_window = 11\n",
    "    alpha = 0.1\n",
    "    input = \"DeepCoffea_data/DCF_data/\"\n",
    "    model = \"./trained_models\"\n",
    "    test = \"./testing_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized loss\n",
    "def cosine_triplet_loss(X):\n",
    "    _alpha = alpha_value\n",
    "    positive_sim, negative_sim = X\n",
    "\n",
    "    losses = K.maximum(0.0, negative_sim - positive_sim + _alpha)\n",
    "    # if similarity is based on the distance functions, use below\n",
    "    # losses = K.maximum(0.0, positive_sim - negative_sim + _alpha)\n",
    "    return K.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "    parser.add_argument ('--tor_len', required=False, default=500)\n",
    "    parser.add_argument ('--exit_len', required=False, default=800)\n",
    "    parser.add_argument ('--win_interval', required=False, default=5)\n",
    "    parser.add_argument ('--num_window', required=False, default=11)\n",
    "    parser.add_argument ('--alpha', required=False, default=0.1)\n",
    "    parser.add_argument ('--input', required=False, default='/data/website-fingerprinting/datasets/new_dcf_data/crawle_new_overlap_interval')\n",
    "    parser.add_argument ('--test', required=False, default='/data/seoh/DeepCCA_model/crawle_overlap_new2021_interal')\n",
    "    parser.add_argument ('--model', required=False, default=\"/data/seoh/DeepCCA_model/crawle_overlap_new2021_\")\n",
    "    args = parser.parse_args ()\n",
    "    return args\n",
    "    \n",
    "\n",
    "args = Args()\n",
    "ktf.set_session(get_session())\n",
    "\n",
    "model_gb = 'cnn1d'\n",
    "\n",
    "## Params for time-based window\n",
    "interval = args.win_interval#5\n",
    "t_flow_size = int(args.tor_len)#500#400#238  # 238#150#184  # 238#264\n",
    "e_flow_size = int(args.exit_len)#800#330#140\n",
    "num_windows = int(args.num_window)#11#21#5\n",
    "window_index_list = np.arange(num_windows)\n",
    "\n",
    "pad_t = t_flow_size * 2\n",
    "pad_e = e_flow_size * 2\n",
    "\n",
    "alpha_value = float(args.alpha)#0.1\n",
    "\n",
    "train_windows1 = []\n",
    "valid_windows1 = []\n",
    "test_windows1 = []\n",
    "train_windows2 = []\n",
    "valid_windows2 = []\n",
    "test_windows2 = []\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "valid_labels = []\n",
    "\n",
    "for window_index in window_index_list:\n",
    "    addn = 2\n",
    "    pickle_path = args.input+str(interval)+'_win'+ str(window_index) +'_addn'+ str(addn) +'_w_superpkt.pickle'\n",
    "\n",
    "\n",
    "    with open (pickle_path, 'rb') as handle:\n",
    "        traces = pickle.load (handle)\n",
    "        tor_seq = traces[\"tor\"]\n",
    "        exit_seq = traces[\"exit\"]\n",
    "        labels = traces[\"label\"]\n",
    "        circuit_labels = np.array ([int (labels[i].split ('_')[0]) for i in range (len (labels))])\n",
    "\n",
    "        print (tor_seq[0])\n",
    "\n",
    "        circuit = {}\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i].split ('_')[0] not in circuit.keys ():\n",
    "                circuit[labels[i].split ('_')[0]] = 1\n",
    "            else:\n",
    "                circuit[labels[i].split ('_')[0]] += 1\n",
    "\n",
    "        # No overlapping circuits between training and testing sets\n",
    "        global test_c\n",
    "        global train_c\n",
    "        if window_index == 0:\n",
    "            test_c = []\n",
    "            train_c = []\n",
    "            sum_ins = 2093\n",
    "            keys = list (circuit.keys ())\n",
    "            random.shuffle (keys)\n",
    "            for key in keys:\n",
    "                if sum_ins > 0:\n",
    "                    sum_ins -= circuit[key]\n",
    "                    test_c.append (key)\n",
    "                else:\n",
    "                    train_c.append (key)\n",
    "            test_c = np.array (test_c).astype ('int')\n",
    "            train_c = np.array (train_c).astype ('int')\n",
    "        # print (train_c)\n",
    "        print ('test_c', test_c)\n",
    "        print ('train_c', train_c)\n",
    "    ###########\n",
    "    train_set_x1, train_set_x2, test_set_x1, test_set_x2, valid_set_x1, valid_set_x2 = load_whole_seq_new(tor_seq,exit_seq,circuit_labels,test_c,train_c,model_gb)\n",
    "\n",
    "    temp_test1 = []\n",
    "    temp_test2 = []\n",
    "\n",
    "    print(train_set_x1.shape)\n",
    "    print(valid_set_x1.shape)\n",
    "    print(test_set_x1.shape)\n",
    "\n",
    "    print('train_set_x1', train_set_x1.shape)\n",
    "    for x in train_set_x1:\n",
    "        train_windows1.append(np.reshape(np.pad(x[:pad_t], (0, pad_t - len(x[:pad_t])), 'constant'), [-1, 1]))\n",
    "\n",
    "    for x in valid_set_x1:\n",
    "        valid_windows1.append(np.reshape(np.pad(x[:pad_t], (0, pad_t - len(x[:pad_t])), 'constant'), [-1, 1]))\n",
    "\n",
    "    for x in test_set_x1:\n",
    "        temp_test1.append(np.reshape(np.pad(x[:pad_t], (0, pad_t - len(x[:pad_t])), 'constant'), [-1, 1]))\n",
    "\n",
    "    for x in train_set_x2:\n",
    "        train_windows2.append(np.reshape(np.pad(x[:pad_e], (0, pad_e - len(x[:pad_e])), 'constant'), [-1, 1]))\n",
    "\n",
    "    for x in valid_set_x2:\n",
    "        valid_windows2.append(np.reshape(np.pad(x[:pad_e], (0, pad_e - len(x[:pad_e])), 'constant'), [-1, 1]))\n",
    "\n",
    "    for x in test_set_x2:\n",
    "        temp_test2.append(np.reshape(np.pad(x[:pad_e], (0, pad_e - len(x[:pad_e])), 'constant'), [-1, 1]))\n",
    "\n",
    "    print('temp_test1: ', np.array(temp_test1).shape)\n",
    "    print('temp_test2: ', np.array(temp_test2).shape)\n",
    "    test_windows1.append(np.array(temp_test1))\n",
    "    test_windows2.append(np.array(temp_test2))\n",
    "\n",
    "np.savez_compressed(args.test+str(interval)+'_test' + str(num_windows) + 'addn'+str(addn)+'_w_superpkt.npz',\n",
    "         tor=np.array(test_windows1),\n",
    "         exit=np.array(test_windows2))\n",
    "\n",
    "train_windows1 = np.array(train_windows1)\n",
    "valid_windows1 = np.array(valid_windows1)\n",
    "\n",
    "train_windows2 = np.array(train_windows2)\n",
    "valid_windows2 = np.array(valid_windows2)\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "valid_labels = np.array(valid_labels)\n",
    "\n",
    "print('train_windows1: ', np.array(train_windows1).shape)\n",
    "print('train_windows2: ', np.array(train_windows2).shape)\n",
    "print('test_windows1: ', np.array(test_windows1).shape)\n",
    "print('test_windows2: ', np.array(test_windows2).shape)\n",
    "\n",
    "input_shape1 = (pad_t, 1)\n",
    "input_shape2 = (pad_e, 1)\n",
    "\n",
    "shared_model1 = create_model(input_shape=input_shape1, emb_size=64, model_name='tor')  ##\n",
    "shared_model2 = create_model(input_shape=input_shape2, emb_size=64, model_name='exit')  ##\n",
    "\n",
    "anchor = Input(input_shape1, name='anchor')\n",
    "positive = Input(input_shape2, name='positive')\n",
    "negative = Input(input_shape2, name='negative')\n",
    "\n",
    "a = shared_model1(anchor)\n",
    "p = shared_model2(positive)\n",
    "n = shared_model2(negative)\n",
    "\n",
    "print('a shape', a.shape)\n",
    "print('p shape', p.shape)\n",
    "print('n shape', n.shape)\n",
    "pos_sim = Dot(axes=-1, normalize=True)([a, p])\n",
    "neg_sim = Dot(axes=-1, normalize=True)([a, n])\n",
    "print('pos_sim shape', pos_sim.shape)\n",
    "print('neg_sim shape', neg_sim.shape)\n",
    "\n",
    "\n",
    "loss = Lambda(cosine_triplet_loss, output_shape=(1,))([pos_sim, neg_sim])\n",
    "\n",
    "model_triplet = Model(\n",
    "    inputs=[anchor, positive, negative],\n",
    "    outputs=loss)\n",
    "print(model_triplet.summary())\n",
    "\n",
    "opt = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)\n",
    "\n",
    "\n",
    "model_triplet.compile(loss=identity_loss, optimizer=opt)\n",
    "\n",
    "batch_size = 64  # batch_size_value\n",
    "\n",
    "\n",
    "def intersect(a, b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "\n",
    "# def build_similarities(conv1, conv2, tor_t, exit_t):\n",
    "\n",
    "#     tor_embs = conv1.predict(tor_t)\n",
    "#     exit_embs = conv2.predict(exit_t)\n",
    "#     all_embs = np.concatenate((tor_embs, exit_embs), axis=0)\n",
    "#     all_embs = all_embs / np.linalg.norm(all_embs, axis=-1, keepdims=True)\n",
    "#     mid = int(len(all_embs) / 2)\n",
    "#     all_sims = np.dot(all_embs[:mid], all_embs[mid:].T)\n",
    "#     return all_sims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(epoch, logs):\n",
    "    global best_loss\n",
    "\n",
    "    loss = logs['loss']\n",
    "\n",
    "    if loss < best_loss:\n",
    "        print(\"loss is improved from {} to {}. save the model\".format(str(best_loss),\n",
    "                                                                      str(loss)))\n",
    "\n",
    "        best_loss = loss\n",
    "        shared_model1.save_weights(\n",
    "            args.model + str(num_windows) + \"_interval\"+str(interval)+ '_addn'+str(addn)+\"_model1_w_superpkt.h5\")\n",
    "        shared_model2.save_weights(\n",
    "            args.model + str(num_windows) + \"_interval\"+str(interval)+'_addn'+str(addn)+\"_model2_w_superpkt.h5\")\n",
    "    else:\n",
    "        print(\"loss is not improved from {}.\".format(str(best_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_negatives(anc_idxs, pos_idxs, similarities, neg_imgs_idx, num_retries=50):\n",
    "    # If no similarities were computed, return a random negative\n",
    "    if similarities is None:\n",
    "        # print(neg_imgs_idx)\n",
    "        # print(anc_idxs)\n",
    "        anc_idxs = list(anc_idxs)\n",
    "        valid_neg_pool = neg_imgs_idx  # .difference(anc_idxs)\n",
    "        print('valid_neg_pool', valid_neg_pool.shape)\n",
    "        return np.random.choice(valid_neg_pool, len(anc_idxs), replace=False)\n",
    "    final_neg = []\n",
    "    # for each positive pair\n",
    "    for (anc_idx, pos_idx) in zip(anc_idxs, pos_idxs):\n",
    "        anchor_class = anc_idx\n",
    "        # print('anchor_class',anchor_class)\n",
    "        valid_neg_pool = neg_imgs_idx  # .difference(np.array([anchor_class]))\n",
    "        # positive similarity\n",
    "        sim = similarities[anc_idx, pos_idx]\n",
    "        # find all negatives which are semi(hard)\n",
    "        possible_ids = np.where((similarities[anc_idx] + alpha_value) > sim)[0]\n",
    "        possible_ids = intersect(valid_neg_pool, possible_ids)\n",
    "        appended = False\n",
    "        for iteration in range(num_retries):\n",
    "            if len(possible_ids) == 0:\n",
    "                break\n",
    "            idx_neg = np.random.choice(possible_ids, 1)[0]\n",
    "            if idx_neg != anchor_class:\n",
    "                final_neg.append(idx_neg)\n",
    "                appended = True\n",
    "                break\n",
    "        if not appended:\n",
    "            final_neg.append(np.random.choice(valid_neg_pool, 1)[0])\n",
    "    return final_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At first epoch we don't generate hard triplets\n",
    "all_traces_train_idx = np.array(range(len(train_windows1)))\n",
    "gen_hard = SemiHardTripletGenerator(train_windows1, train_windows2, batch_size, all_traces_train_idx,\n",
    "                                    train_windows1, train_windows2, None, None)\n",
    "nb_epochs = 10000\n",
    "description = 'coffeh2'\n",
    "\n",
    "best_loss = sys.float_info.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking_dot(big_matrix, small_matrix, chunk_size=100):\n",
    "    # Create a memory-mapped array to store the result\n",
    "    result_shape = (big_matrix.shape[0], small_matrix.shape[1])\n",
    "    result = np.memmap(\"result_array\", dtype=np.float32, mode='w+', shape=result_shape)\n",
    "    # Make a copy if the array is not already contiguous\n",
    "    small_matrix = np.ascontiguousarray(small_matrix)\n",
    "    for i in range(0, result.shape[0], chunk_size):\n",
    "        end = i + chunk_size\n",
    "        result[i:end] = np.dot(big_matrix[i:end], small_matrix)\n",
    "    return result\n",
    "    \n",
    "def build_similarities(conv1, conv2, tor_t, exit_t):\n",
    "\n",
    "    tor_embs = conv1.predict(tor_t)\n",
    "    exit_embs = conv2.predict(exit_t)\n",
    "    all_embs = np.concatenate((tor_embs, exit_embs), axis=0)\n",
    "    all_embs = all_embs / np.linalg.norm(all_embs, axis=-1, keepdims=True)\n",
    "    mid = int(len(all_embs) / 2)\n",
    "    all_sims = chunking_dot(all_embs[:mid], all_embs[mid:].T)\n",
    "    return all_sims\n",
    "\n",
    "class SemiHardTripletGenerator():\n",
    "    def __init__(self, Xa_train, Xp_train, batch_size, neg_traces_train_idx, Xa_train_all, Xp_train_all, conv1,\n",
    "                 conv2):\n",
    "        self.batch_size = batch_size  # 128\n",
    "\n",
    "        self.Xa = Xa_train\n",
    "        self.Xp = Xp_train\n",
    "        self.Xa_all = Xa_train_all\n",
    "        self.Xp_all = Xp_train_all\n",
    "        self.Xp = Xp_train\n",
    "        self.cur_train_index = 0\n",
    "        self.num_samples = Xa_train.shape[0]\n",
    "        self.neg_traces_idx = neg_traces_train_idx\n",
    "\n",
    "        if conv1:\n",
    "            self.similarities = build_similarities(conv1, conv2, self.Xa_all,self.Xp_all)\n",
    "        else:\n",
    "            self.similarities = None\n",
    "\n",
    "    def next_train(self):\n",
    "        while 1:\n",
    "            self.cur_train_index += self.batch_size\n",
    "            if self.cur_train_index >= self.num_samples:\n",
    "                self.cur_train_index = 0  # initialize the index for the next epoch\n",
    "\n",
    "            # fill one batch\n",
    "            traces_a = np.array(range(self.cur_train_index,\n",
    "                                      self.cur_train_index + self.batch_size))\n",
    "            traces_p = np.array(range(self.cur_train_index,\n",
    "                                      self.cur_train_index + self.batch_size))\n",
    "\n",
    "            traces_n = build_negatives(traces_a, traces_p, self.similarities, self.neg_traces_idx)\n",
    "            yield ([self.Xa[traces_a],\n",
    "                    self.Xp[traces_p],\n",
    "                    self.Xp_all[traces_n]],\n",
    "                   np.zeros(shape=(traces_a.shape[0]))\n",
    "                   )\n",
    "            \n",
    "for epoch in range(nb_epochs):\n",
    "    print(\"built new hard generator for epoch \" + str(epoch))\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        if epoch == 0:\n",
    "            model_triplet.fit_generator(generator=gen_hard.next_train(),\n",
    "                                        steps_per_epoch=train_windows1.shape[0] // batch_size - 1,\n",
    "                                        epochs=1, verbose=1)\n",
    "        else:\n",
    "            model_triplet.fit_generator(generator=gen_hard_even.next_train(),\n",
    "                                        steps_per_epoch=(train_windows1.shape[0] // 2) // batch_size - 1,\n",
    "                                        epochs=1, verbose=1, callbacks=[LambdaCallback(on_epoch_end=saveModel)])\n",
    "    else:\n",
    "        model_triplet.fit_generator(generator=gen_hard_odd.next_train(),\n",
    "                                    steps_per_epoch=(train_windows1.shape[0] // 2) // batch_size - 1,\n",
    "                                    epochs=1, verbose=1, callbacks=[LambdaCallback(on_epoch_end=saveModel)])\n",
    "\n",
    "    mid = int(len(train_windows1) / 2)\n",
    "    random_ind = np.array(range(len(train_windows1)))\n",
    "    np.random.shuffle(random_ind)\n",
    "    X1 = np.array(random_ind[:mid])\n",
    "    X2 = np.array(random_ind[mid:])\n",
    "\n",
    "    gen_hard_odd = SemiHardTripletGenerator(train_windows1[X1], train_windows2[X1], batch_size, X2, train_windows1,train_windows2,shared_model1, shared_model2)\n",
    "    gen_hard_even = SemiHardTripletGenerator(train_windows1[X2], train_windows2[X2], batch_size,X1, train_windows1, train_windows2,shared_model1, shared_model2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def compute_similarity_with_memmap(conv1, conv2, tor_t, exit_t, memmap_batch_size):\n",
    "    # Get the shapes of the embeddings\n",
    "    tor_shape = conv1.predict(tor_t).shape\n",
    "    exit_shape = conv2.predict(exit_t).shape\n",
    "    \n",
    "    print(\"tor shape\", tor_shape)\n",
    "    print(\"exit shape\", exit_shape)\n",
    "\n",
    "    # Define the result shape based on the shapes of tor_embs and exit_embs\n",
    "    result_shape = (exit_shape[0], tor_shape[0])\n",
    "\n",
    "    # Create a memory-mapped array to store the result\n",
    "    result = np.memmap(\"all_sims\", dtype=np.float32, mode='w+', shape=result_shape)\n",
    "\n",
    "    # Calculate embeddings for tor_t and exit_t\n",
    "    tor_embs = conv1.predict(tor_t)\n",
    "    exit_embs = conv2.predict(exit_t)\n",
    "\n",
    "    # Concatenate embeddings\n",
    "    all_embs = np.concatenate((tor_embs, exit_embs), axis=0)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    all_embs = all_embs / np.linalg.norm(all_embs, axis=-1, keepdims=True)\n",
    "\n",
    "    mid = int(len(all_embs) / 2)\n",
    "\n",
    "    # Use tqdm for the outer loop to display progress\n",
    "    with tqdm(total=mid, desc=\"Calculating similarity\", unit=\"batch\") as pbar_outer:\n",
    "        for i in range(0, mid, memmap_batch_size):\n",
    "            pbar_outer.update(1)  # Update progress bar for each batch\n",
    "\n",
    "            # Use tqdm for the inner loop to display progress\n",
    "            with tqdm(total=len(all_embs), desc=\"Progress\", unit=\"batch\", leave=False) as pbar_inner:\n",
    "                for j in range(mid, len(all_embs), memmap_batch_size):\n",
    "                    # Perform dot product on smaller chunks\n",
    "                    chunk_result = np.dot(all_embs[i:i+memmap_batch_size], all_embs[j:j+memmap_batch_size].T)\n",
    "                    \n",
    "                    print(f\"i: {i}, j: {j}\")\n",
    "                    print(i,i+memmap_batch_size)\n",
    "                    print(j,j+memmap_batch_size)\n",
    "                    print(f\"result slice shape: {result[i:i+memmap_batch_size, j:j+memmap_batch_size].shape}\")\n",
    "                    print(\"result shape: \", result.shape)\n",
    "                    print(\"chunk result content\", chunk_result.shape)\n",
    "                    \n",
    "                    result[i:i+memmap_batch_size, j:j+memmap_batch_size] = chunk_result\n",
    "                    pbar_inner.update(1)  # Update progress bar for each batch\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
